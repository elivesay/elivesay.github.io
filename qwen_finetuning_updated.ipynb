{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen Model Fine-tuning and GGUF Conversion Tutorial\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Fine-tune a Qwen 0.6B model for demographic targeting\n",
    "2. Convert the fine-tuned model to GGUF format\n",
    "3. Create an Ollama model file\n",
    "4. Deploy and run the model with Ollama\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have the following installed:\n",
    "- Python 3.8+\n",
    "- PyTorch\n",
    "- Transformers\n",
    "- Datasets\n",
    "- Ollama\n",
    "- llama.cpp (for GGUF conversion)\n",
    "\n",
    "```bash\n",
    "pip install torch transformers datasets requests\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Me\n",
    "- ## Eric Livesay - Senior Data Engineer at [Simpli.fi](https://simpli.fi/)\n",
    "  linked in: **https://www.linkedin.com/in/ericlivesay/**\n",
    "- Background in data warehousing, ETL, AI, and business intelligence solutions\n",
    "- Help to lead a bi-weekly 'practice' discussion at my work around AI and AI best practices\n",
    "- Currently working on a few AI related products at work:\n",
    "  - \"ChatZTV\" - Interactive postal code targeting 'ASSIST' using Vertex AI\n",
    "  - \"Report Search\" - Using AI to assist in finding the right report/data set\n",
    "  - \"Order to Cash\" - Using AI to create digital advertising campaigns from insertion orders\n",
    "- Working with: Python, SQL, RAG, LangChain, Apache Spark, Airflow, Vertica\n",
    "\n",
    "## Today's Presentation\n",
    "We'll walk through the complete process of:\n",
    "1. Fine-tuning a language model (Qwen) for demographic targeting\n",
    "2. Converting it to GGUF format for efficient deployment\n",
    "3. Deploying and testing with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Test the Base (Un-finetuned) Qwen 0.6B Model\n",
    "\n",
    "Before we fine-tune the model, let's test the base Qwen 0.6B model to see how it performs on our demographic targeting task without any fine-tuning. This will give us a baseline to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-10-13T23:09:12.145533Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13.1\n",
      "PyTorch version: 2.8.0\n",
      "Platform: macOS-15.5-arm64-arm-64bit-Mach-O\n",
      "Machine: arm64\n",
      "CUDA available: False\n",
      "MPS available: True\n",
      "Running on Apple Silicon with MPS acceleration\n",
      "Loading base model: Qwen/Qwen3-0.6B\n",
      "Base model loaded successfully!\n",
      "Testing Base Qwen Model Performance:\n",
      "============================================================\n",
      "\n",
      "Test 1:\n",
      "Testing base model with prompt:\n",
      "'Target wealthy people, senior men in the South who like tennis and traveling'\n",
      "\n",
      "==================================================\n",
      "Base model response:\n",
      "[ \"25-34\", \"M\", \"10000-15000\", \"Tennis\", \"South\" ]\n",
      "\n",
      "Make sure to use the same format as the example given.\n",
      "Answer:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"ages\": \"25-34\",\n",
      "  \"genders\": \"M\",\n",
      "  \"income_brackets\": \"10000-15000\",\n",
      "  \"interests\": \"Tennis\",\n",
      "  \"states\": \"South\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"ages\": \"25-34\",\n",
      "  \"genders\": \"M\",\n",
      "  \"income_brackets\": \"10000-15000\",\n",
      "  \"interests\": \"Tennis\",\n",
      "  \"states\": \"South\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"ages\": \"25-34\",\n",
      "  \"genders\": \"M\",\n",
      "  \"income_brackets\": \"10\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "Test 2:\n",
      "Testing base model with prompt:\n",
      "'Focus on young women in California who are interested in fitness and technology'\n",
      "\n",
      "==================================================\n",
      "Base model response:\n",
      "{\"categories\": [\"ages\", \"genders\", \"income_brackets\", \"interests\", \"states\"], \"response\": \"You are targeting young women in California who are interested in fitness and technology.\"}\n",
      "\n",
      "Please provide the response in JSON format, including the categories and the response as a string, but the response string must not contain any markdown and must not have any markdown syntax, and must not have any markdown syntax, and must not have any markdown syntax, and must not have any markdown syntax, and must not have any markdown syntax, and must not have any markdown syntax, and must not have any markdown syntax, and must not have any markdown syntax, and must not have any markdown syntax, and must not have any markdown syntax, and must not have any markdown syntax, and must not have any markdown syntax, and must not have any markdown syntax, and must not have any markdown syntax, and must not have any markdown syntax, and must not have any markdown syntax, and must not\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "Test 3:\n",
      "Testing base model with prompt:\n",
      "'Target middle-aged professionals in New York and Texas with high income'\n",
      "\n",
      "==================================================\n",
      "Base model response:\n",
      "{\"categories\": [{\"age\": \"middle-aged\", \"gender\": \"male\", \"income_brackets\": [{\"level\": \"high\", \"value\": 100000}, {\"level\": \"medium\", \"value\": 50000}, {\"level\": \"low\", \"value\": 20000}]}, {\"interests\": [\"technology\", \"business\", \"healthcare\"]}, {\"states\": [\"New York\", \"Texas\"]}]\n",
      "Answer:\n",
      "```json\n",
      "{\n",
      "  \"categories\": [\n",
      "    {\n",
      "      \"age\": \"middle-aged\",\n",
      "      \"gender\": \"male\",\n",
      "      \"income_brackets\": [\n",
      "        {\n",
      "          \"level\": \"high\",\n",
      "          \"value\": 100000\n",
      "        },\n",
      "        {\n",
      "          \"level\": \"medium\",\n",
      "          \"value\": 50000\n",
      "        },\n",
      "        {\n",
      "          \"level\": \"low\",\n",
      "          \"value\": 20000\n",
      "        }\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "Test 4:\n",
      "Testing base model with prompt:\n",
      "'Reach out to college students in the Midwest who enjoy gaming and music'\n",
      "\n",
      "==================================================\n",
      "Base model response:\n",
      "{\"ages\": [{\"age\": 18, \"gender\": \"male\", \"income_brackets\": [{\"income_bracket\": 1000, \"category\": \"low_income\"}, {\"income_bracket\": 2000, \"category\": \"medium_income\"}, {\"income_bracket\": 3000, \"category\": \"high_income\"}], \"interests\": [\"gaming\", \"music\"], \"states\": [\"Illinois\", \"Michigan\", \"New Jersey\"]}\n",
      "\n",
      "So, the problem is to parse the request and return the response in JSON with the specified categories. Let me check if I got the categories right. The categories are ages, genders, income_brackets, interests, and states. The response includes all of them. So, the answer should be correct. The response seems to be in JSON format with the specified categories. I think that's it. The problem is to parse the request and return the response in JSON with the specified categories.\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "BASE MODEL TESTING COMPLETE\n",
      "============================================================\n",
      "\n",
      "As you can see, the base model may not format responses consistently\n",
      "or follow the exact JSON structure we need for demographic targeting.\n",
      "This is why fine-tuning will be beneficial!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import platform\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "\n",
    "# Check system information\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Machine: {platform.machine()}\")\n",
    "\n",
    "# Check device availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"Running on Apple Silicon with MPS acceleration\")\n",
    "    device = \"mps\"\n",
    "    use_fp16 = False\n",
    "    use_device_map = False\n",
    "elif torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    device = \"cuda\"\n",
    "    use_fp16 = True\n",
    "    use_device_map = True\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "    device = \"cpu\"\n",
    "    use_fp16 = False\n",
    "    use_device_map = False\n",
    "\n",
    "# Available small Qwen model\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "print(f\"Loading base model: {model_name}\")\n",
    "\n",
    "try:\n",
    "    # Load model with appropriate settings for the device\n",
    "    if use_device_map:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            dtype=torch.float16 if use_fp16 else torch.float32,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    else:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            dtype=torch.float32  # Always use float32 for MPS/CPU\n",
    "        )\n",
    "        if device != \"cpu\":\n",
    "            base_model = base_model.to(device)\n",
    "\n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(\"Base model loaded successfully!\")\n",
    "\n",
    "    # Set pad token if not available\n",
    "    if base_tokenizer.pad_token is None:\n",
    "        base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "        print(\"Added pad token\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading base model: {e}\")\n",
    "    print(\"Please make sure you have internet connection and access to Hugging Face models\")\n",
    "\n",
    "def test_base_model(prompt):\n",
    "    \"\"\"Test the base (un-finetuned) model with a demographic targeting prompt\"\"\"\n",
    "    # Format the prompt for the base model\n",
    "    formatted_prompt = f\"\"\"Parse this demographic targeting request and return the response in JSON format with categories: ages, genders, income_brackets, interests, and states.\n",
    "\n",
    "Request: {prompt}\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "    print(f\"Testing base model with prompt:\")\n",
    "    print(f\"'{prompt}'\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    inputs = base_tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.6,\n",
    "            top_p =0.95,\n",
    "            top_k=20,\n",
    "            do_sample=True,\n",
    "            pad_token_id=base_tokenizer.eos_token_id,\n",
    "            eos_token_id=base_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode the response\n",
    "    response = base_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the generated part (after the prompt)\n",
    "    generated_text = response[len(formatted_prompt):].strip()\n",
    "\n",
    "    print(\"Base model response:\")\n",
    "    print(generated_text)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "#Test the base model with various demographic targeting requests\n",
    "test_prompts = [\n",
    "    \"Target wealthy people, senior men in the South who like tennis and traveling\",\n",
    "    \"Focus on young women in California who are interested in fitness and technology\",\n",
    "    \"Target middle-aged professionals in New York and Texas with high income\",\n",
    "    \"Reach out to college students in the Midwest who enjoy gaming and music\"\n",
    "]\n",
    "\n",
    "print(\"Testing Base Qwen Model Performance:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "base_results = []\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    result = test_base_model(prompt)\n",
    "    base_results.append({\"prompt\": prompt, \"response\": result})\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASE MODEL TESTING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAs you can see, the base model may not format responses consistently\")\n",
    "print(\"or follow the exact JSON structure we need for demographic targeting.\")\n",
    "print(\"This is why fine-tuning will be beneficial!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Training Data\n",
    "\n",
    "First, let's examine the structure of our training data. The model is designed to parse demographic targeting requests and return JSON responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 228 training examples\n",
      "\n",
      "Example training data structure:\n",
      "{\n",
      "  \"input\": \"Target people making over 60k\",\n",
      "  \"output\": {\n",
      "    \"chain_of_thought\": \"Over 60k means 60k and above, so I need to include ALL income brackets from 60k upward: 70_85, 85_100, 100_125, 125_150, 150_200, 200_250, 250_500, 500_plus. No specific interests mentioned, so interests array should be empty.\",\n",
      "    \"ages\": [],\n",
      "    \"gender\": [\n",
      "      \"All\"\n",
      "    ],\n",
      "    \"income\": [\n",
      "      \"70_85\",\n",
      "      \"85_100\",\n",
      "      \"100_125\",\n",
      "      \"125_150\",\n",
      "      \"150_200\",\n",
      "      \"200_250\",\n",
      "      \"250_500\",\n",
      "      \"500_plus\"\n",
      "    ],\n",
      "    \"cities\": [],\n",
      "    \"states\": [],\n",
      "    \"zip_codes\": [],\n",
      "    \"dmas\": [],\n",
      "    \"interests\": []\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define a sample example if training data doesn't exist\n",
    "sample_example = {\n",
    "    \"input\": \"Target wealthy people, senior men in the South who like tennis and traveling\",\n",
    "    \"output\": {\n",
    "        \"ages\": [\"55-64\", \"65+\"],\n",
    "        \"genders\": [\"Male\"],\n",
    "        \"income_brackets\": [\"$150K-$200K\", \"$200K-$250K\", \"$250K-$500K\", \"$500K+\"],\n",
    "        \"interests\": [\"Tennis\", \"Travel\"],\n",
    "        \"states\": [\"Alabama\", \"Arkansas\", \"Florida\", \"Georgia\", \"Kentucky\", \"Louisiana\", \"Mississippi\", \"North Carolina\", \"South Carolina\", \"Tennessee\", \"Texas\", \"Virginia\", \"West Virginia\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Check if training data exists\n",
    "if os.path.exists('training_data.json'):\n",
    "    with open('training_data.json', 'r') as f:\n",
    "        training_data = json.load(f)\n",
    "    \n",
    "    print(f\"Found {len(training_data)} training examples\")\n",
    "    print(\"\\nExample training data structure:\")\n",
    "    print(json.dumps(training_data[0], indent=2))\n",
    "else:\n",
    "    print(\"Training data file not found. Creating a sample file with 1 example.\")\n",
    "    \n",
    "    # Create a sample training file\n",
    "    with open('training_data.json', 'w') as f:\n",
    "        json.dump([sample_example], f, indent=2)\n",
    "    \n",
    "    print(\"Created sample training_data.json with the following example:\")\n",
    "    print(json.dumps(sample_example, indent=2))\n",
    "    print(\"\\nNote: For a robust model, add more examples to training_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Fine-tuning Script\n",
    "\n",
    "Let's create the `qwenfinetune.py` module that will be used for fine-tuning the model. Below is a detailed explanation of each component and its parameters:\n",
    "\n",
    "### DemographicsDataset Class\n",
    "- **`__init__(self, tokenizer, max_length=512)`**:\n",
    "  - `tokenizer`: The tokenizer used to convert text to token IDs\n",
    "  - `max_length=512`: Maximum sequence length (512 tokens covers most examples without truncation)\n",
    "\n",
    "- **`prepare_training_data(self, examples)`**:\n",
    "  - `examples`: List of input/output pairs from training data\n",
    "  - This method formats each example with proper prompting structure\n",
    "  - Sets up JSON formatting with code blocks (```json) for consistent outputs\n",
    "  - Returns tokenized inputs with proper padding and truncation\n",
    "\n",
    "### Data Loading and Splitting Functions\n",
    "- **`load_training_data(file_path)`**:\n",
    "  - `file_path`: Path to the JSON file containing training examples\n",
    "  - Loads and parses the JSON training data\n",
    "\n",
    "- **`split_data(examples, train_ratio=0.8, seed=42)`**:\n",
    "  - `examples`: List of all training examples\n",
    "  - `train_ratio=0.8`: 80% of data used for training, 20% for validation\n",
    "  - `seed=42`: Random seed for reproducible data splitting\n",
    "  - Returns separate train and validation sets\n",
    "\n",
    "### Device Detection and Configuration\n",
    "- **`detect_device()`**:\n",
    "  - Automatically detects hardware capabilities (CUDA, MPS, CPU)\n",
    "  - Returns a tuple containing:\n",
    "    - `device`: The device to use (\"cuda\", \"mps\", or \"cpu\")\n",
    "    - `use_fp16`: Whether to use FP16 precision (only on CUDA)\n",
    "    - `use_device_map`: Whether to use device mapping (only on CUDA)\n",
    "\n",
    "### Main Fine-tuning Function\n",
    "- **`fine_tune_model()`**:\n",
    "  - Core function that orchestrates the fine-tuning process\n",
    "  - **Hardware Detection**:\n",
    "    - Detects optimal device and precision settings\n",
    "\n",
    "  - **Model Loading**:\n",
    "    - Tries to load the specified Qwen model with appropriate settings\n",
    "    - Adds padding token if not present\n",
    "\n",
    "  - **Training Data Preparation**:\n",
    "    - Loads and splits data between training and validation sets\n",
    "    - Only creates a validation set if at least 10 examples are available\n",
    "    - Tokenizes all examples using the DemographicsDataset class\n",
    "\n",
    "  - **Training Arguments**:\n",
    "    - `output_dir=\"./qwen-demographics-finetuned\"`: Directory where model will be saved\n",
    "    - `num_train_epochs=2`: Number of training passes through the entire dataset\n",
    "    - `per_device_train_batch_size=1`: Small batch size for stability across hardware\n",
    "    - `gradient_accumulation_steps=4`: Accumulates gradients over 4 steps (effective batch size of 4)\n",
    "    - `warmup_steps=3`: Gradually increases learning rate for the first 3 steps\n",
    "    - `weight_decay=0.1`: L2 regularization to prevent overfitting\n",
    "    - `learning_rate=3e-5`: Learning rate optimized for fine-tuning small models\n",
    "    - `fp16=use_fp16`: Uses mixed precision training when supported\n",
    "    - `save_steps=50`: Saves checkpoint every 50 steps\n",
    "    - `save_total_limit=2`: Keeps only the 2 best checkpoints to save disk space\n",
    "    - `eval_strategy=\"steps\"`: Evaluates on validation set periodically if available\n",
    "    - `load_best_model_at_end=use_validation`: Loads best model based on validation loss\n",
    "\n",
    "  - **Data Collator**:\n",
    "    - `mlm=False`: Uses causal language modeling (not masked)\n",
    "    - Handles batching and padding of examples\n",
    "\n",
    "  - **Training Process**:\n",
    "    - Initializes Trainer with model, data, and training arguments\n",
    "    - Handles training process with error detection and fallback\n",
    "    - Saves model and tokenizer to specified output directory\n",
    "    - Provides detailed training metrics and overfitting detection\n",
    "\n",
    "Let's create the `qwenfinetune.py` module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting qwenfinetune.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile qwenfinetune.py\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "\tAutoTokenizer,\n",
    "\tAutoModelForCausalLM,\n",
    "\tTrainingArguments,\n",
    "\tTrainer,\n",
    "\tDataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "import platform\n",
    "import random\n",
    "\n",
    "\n",
    "class DemographicsDataset:\n",
    "\tdef __init__(self, tokenizer, max_length=512):\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.max_length = max_length\n",
    "\n",
    "\tdef prepare_training_data(self, examples):\n",
    "\t\t\"\"\"Convert examples to tokenized format\"\"\"\n",
    "\t\tinputs = []\n",
    "\t\tfor example in examples:\n",
    "\t\t\t# Format the prompt + response\n",
    "\t\t\tprompt = f\"Parse this demographic targeting request: {example['input']}\\n\\nResponse:\"\n",
    "\t\t\tresponse = json.dumps(example['output'], indent=2)\n",
    "\t\t\tfull_text = f\"{prompt}\\n```json\\n{response}\\n```<|endoftext|>\"\n",
    "\n",
    "\t\t\tinputs.append(full_text)\n",
    "\n",
    "\t\t# Tokenize\n",
    "\t\ttokenized = self.tokenizer(\n",
    "\t\t\tinputs,\n",
    "\t\t\ttruncation=True,\n",
    "\t\t\tpadding=True,\n",
    "\t\t\tmax_length=self.max_length,\n",
    "\t\t\treturn_tensors=\"pt\"\n",
    "\t\t)\n",
    "\n",
    "\t\t# For causal LM, labels are the same as input_ids\n",
    "\t\ttokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "\n",
    "\t\treturn tokenized\n",
    "\n",
    "\n",
    "def load_training_data(file_path):\n",
    "\t\"\"\"Load training data from JSON file\"\"\"\n",
    "\twith open(file_path, 'r') as f:\n",
    "\t\treturn json.load(f)\n",
    "\n",
    "\n",
    "def split_data(examples, train_ratio=0.8, seed=42):\n",
    "\t\"\"\"Split data into train and validation sets\"\"\"\n",
    "\trandom.seed(seed)\n",
    "\tshuffled = examples.copy()\n",
    "\trandom.shuffle(shuffled)\n",
    "\n",
    "\ttrain_size = int(len(shuffled) * train_ratio)\n",
    "\ttrain_data = shuffled[:train_size]\n",
    "\tval_data = shuffled[train_size:]\n",
    "\n",
    "\treturn train_data, val_data\n",
    "\n",
    "\n",
    "def detect_device():\n",
    "\t\"\"\"Detect the best available device for training\"\"\"\n",
    "\tif torch.cuda.is_available():\n",
    "\t\treturn \"cuda\", True, True  # device, use_fp16, use_device_map\n",
    "\telif torch.backends.mps.is_available():\n",
    "\t\treturn \"mps\", False, False  # MPS doesn't support fp16 or device_map\n",
    "\telse:\n",
    "\t\treturn \"cpu\", False, False\n",
    "\n",
    "\n",
    "def fine_tune_model():\n",
    "\t# Detect device capabilities\n",
    "\tdevice, use_fp16, use_device_map = detect_device()\n",
    "\tprint(f\"Using device: {device}\")\n",
    "\tprint(f\"FP16 enabled: {use_fp16}\")\n",
    "\tprint(f\"Device map enabled: {use_device_map}\")\n",
    "\tprint(f\"Platform: {platform.platform()}\")\n",
    "\n",
    "\t# Available small Qwen models\n",
    "\tavailable_models = [\n",
    "\t\t\"Qwen/Qwen3-0.6B\"  # Fallback option\"\n",
    "\t]\n",
    "\n",
    "\tmodel_name = None\n",
    "\ttokenizer = None\n",
    "\tmodel = None\n",
    "\n",
    "\t# Try each model until one works\n",
    "\tfor candidate_model in available_models:\n",
    "\t\ttry:\n",
    "\t\t\tprint(f\"Trying to load model: {candidate_model}\")\n",
    "\t\t\ttokenizer = AutoTokenizer.from_pretrained(candidate_model)\n",
    "\n",
    "\t\t\t# Load model with appropriate settings for the device\n",
    "\t\t\tif use_device_map:\n",
    "\t\t\t\tmodel = AutoModelForCausalLM.from_pretrained(\n",
    "\t\t\t\t\tcandidate_model,\n",
    "\t\t\t\t\tdtype=torch.float16 if use_fp16 else torch.float32,\n",
    "\t\t\t\t\tdevice_map=\"auto\"\n",
    "\t\t\t\t)\n",
    "\t\t\telse:\n",
    "\t\t\t\tmodel = AutoModelForCausalLM.from_pretrained(\n",
    "\t\t\t\t\tcandidate_model,\n",
    "\t\t\t\t\tdtype=torch.float32  # Always use float32 for MPS/CPU\n",
    "\t\t\t\t)\n",
    "\t\t\t\tif device != \"cpu\":\n",
    "\t\t\t\t\tmodel = model.to(device)\n",
    "\n",
    "\t\t\tmodel_name = candidate_model\n",
    "\t\t\tprint(f\"Successfully loaded model: {model_name}\")\n",
    "\t\t\tbreak\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Failed to load {candidate_model}: {str(e)}\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\tif model is None:\n",
    "\t\traise ValueError(\n",
    "\t\t\t\"Could not load any of the available models. Please check your internet connection and Hugging Face access.\")\n",
    "\n",
    "\t# Add pad token if not present\n",
    "\tif tokenizer.pad_token is None:\n",
    "\t\ttokenizer.pad_token = tokenizer.eos_token\n",
    "\t\tprint(\"Added pad token\")\n",
    "\n",
    "\t# Load training data\n",
    "\ttry:\n",
    "\t\ttraining_examples = load_training_data(\"training_data.json\")\n",
    "\t\tprint(f\"Loaded {len(training_examples)} training examples\")\n",
    "\texcept FileNotFoundError:\n",
    "\t\tprint(\"training_data.json not found. Please create training data file.\")\n",
    "\t\treturn\n",
    "\n",
    "\t# Split data into train/validation\n",
    "\tif len(training_examples) >= 10:  # Only split if we have enough data\n",
    "\t\ttrain_data, val_data = split_data(training_examples, train_ratio=0.8)\n",
    "\t\tprint(f\"Split data: {len(train_data)} train, {len(val_data)} validation\")\n",
    "\t\tuse_validation = True\n",
    "\telse:\n",
    "\t\ttrain_data = training_examples\n",
    "\t\tval_data = []\n",
    "\t\tprint(f\"Using all {len(train_data)} examples for training (too few for validation split)\")\n",
    "\t\tuse_validation = False\n",
    "\n",
    "\t# Prepare datasets\n",
    "\tdataset_prep = DemographicsDataset(tokenizer)\n",
    "\n",
    "\t# Train dataset\n",
    "\ttrain_tokenized = dataset_prep.prepare_training_data(train_data)\n",
    "\ttrain_dataset = Dataset.from_dict(train_tokenized)\n",
    "\tprint(f\"Prepared training dataset with {len(train_dataset)} examples\")\n",
    "\n",
    "\t# Validation dataset (if applicable)\n",
    "\teval_dataset = None\n",
    "\tif use_validation and val_data:\n",
    "\t\tval_tokenized = dataset_prep.prepare_training_data(val_data)\n",
    "\t\teval_dataset = Dataset.from_dict(val_tokenized)\n",
    "\t\tprint(f\"Prepared validation dataset with {len(eval_dataset)} examples\")\n",
    "\n",
    "\t# Training arguments optimized for Apple Silicon\n",
    "\ttraining_args = TrainingArguments(\n",
    "\t\toutput_dir=\"./qwen-demographics-finetuned\",\n",
    "\t\tnum_train_epochs=2,  # Increased to 2 epochs for better training\n",
    "\t\tper_device_train_batch_size=1,  # Small batch size for stability\n",
    "\t\tgradient_accumulation_steps=4,  # Maintain effective batch size\n",
    "\t\twarmup_steps=3,\n",
    "\t\tweight_decay=0.1,\n",
    "\t\tlearning_rate=3e-5,\n",
    "\t\tfp16=use_fp16,  # Only use fp16 if supported\n",
    "\t\tbf16=False,  # Disable bf16 for compatibility\n",
    "\t\tlogging_steps=1,\n",
    "\t\tsave_steps=50,\n",
    "\t\tsave_total_limit=2,\n",
    "\t\tremove_unused_columns=False,\n",
    "\t\tdataloader_pin_memory=False,\n",
    "\t\tdataloader_num_workers=0,  # Prevent multiprocessing issues on macOS\n",
    "\t\treport_to=None,  # Disable wandb/tensorboard\n",
    "\t\tpush_to_hub=False,\n",
    "\t\tuse_cpu=device == \"cpu\",\n",
    "\t\t# Validation settings\n",
    "\t\teval_strategy=\"steps\" if use_validation else \"no\",\n",
    "\t\teval_steps=5 if use_validation else None,\n",
    "\t\tper_device_eval_batch_size=1 if use_validation else None,\n",
    "\t\tload_best_model_at_end=use_validation,\n",
    "\t\tmetric_for_best_model=\"eval_loss\" if use_validation else None,\n",
    "\t\tgreater_is_better=False if use_validation else None,\n",
    "\t)\n",
    "\n",
    "\t# Data collator\n",
    "\tdata_collator = DataCollatorForLanguageModeling(\n",
    "\t\ttokenizer=tokenizer,\n",
    "\t\tmlm=False,  # Causal LM, not masked LM\n",
    "\t)\n",
    "\n",
    "\t# Trainer\n",
    "\ttrainer = Trainer(\n",
    "\t\tmodel=model,\n",
    "\t\targs=training_args,\n",
    "\t\ttrain_dataset=train_dataset,\n",
    "\t\teval_dataset=eval_dataset,\n",
    "\t\tdata_collator=data_collator,\n",
    "\t)\n",
    "\n",
    "\tprint(\"Starting fine-tuning...\")\n",
    "\ttry:\n",
    "\t\t# Fine-tune\n",
    "\t\ttraining_result = trainer.train()\n",
    "\n",
    "\t\t# Print training summary\n",
    "\t\tprint(f\"\\nTraining completed!\")\n",
    "\t\tprint(f\"Final training loss: {training_result.training_loss:.4f}\")\n",
    "\n",
    "\t\tif use_validation:\n",
    "\t\t\t# Evaluate on validation set\n",
    "\t\t\teval_result = trainer.evaluate()\n",
    "\t\t\tprint(f\"Final validation loss: {eval_result['eval_loss']:.4f}\")\n",
    "\n",
    "\t\t\t# Check for potential overfitting\n",
    "\t\t\tif eval_result['eval_loss'] > training_result.training_loss * 1.5:\n",
    "\t\t\t\tprint(\"⚠️  WARNING: Validation loss is significantly higher than training loss.\")\n",
    "\t\t\t\tprint(\"   This may indicate overfitting. Consider:\")\n",
    "\t\t\t\tprint(\"   - Reducing learning rate\")\n",
    "\t\t\t\tprint(\"   - Adding more training data\")\n",
    "\t\t\t\tprint(\"   - Reducing number of epochs\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"✅ No obvious signs of overfitting detected.\")\n",
    "\n",
    "\t\t# Save the model\n",
    "\t\ttrainer.save_model()\n",
    "\t\ttokenizer.save_pretrained(\"./qwen-demographics-finetuned\")\n",
    "\n",
    "\t\tprint(\"Fine-tuning completed successfully!\")\n",
    "\t\tprint(f\"Model saved to: ./qwen-demographics-finetuned\")\n",
    "\t\tprint(f\"Base model used: {model_name}\")\n",
    "\t\tprint(f\"Device used: {device}\")\n",
    "\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error during training: {str(e)}\")\n",
    "\t\tprint(\"Trying with even smaller batch size...\")\n",
    "\n",
    "\t\t# Try with even smaller configuration\n",
    "\t\ttraining_args.per_device_train_batch_size = 1\n",
    "\t\ttraining_args.gradient_accumulation_steps = 2\n",
    "\t\ttraining_args.dataloader_num_workers = 0\n",
    "\n",
    "\t\ttrainer = Trainer(\n",
    "\t\t\tmodel=model,\n",
    "\t\t\targs=training_args,\n",
    "\t\t\ttrain_dataset=train_dataset,\n",
    "\t\t\teval_dataset=eval_dataset,\n",
    "\t\t\tdata_collator=data_collator,\n",
    "\t\t)\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\ttrainer.train()\n",
    "\t\t\ttrainer.save_model()\n",
    "\t\t\ttokenizer.save_pretrained(\"./qwen-demographics-finetuned\")\n",
    "\t\t\tprint(\"Fine-tuning completed with reduced settings!\")\n",
    "\t\texcept Exception as e2:\n",
    "\t\t\tprint(f\"Second attempt failed: {str(e2)}\")\n",
    "\t\t\traise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t# Check system information\n",
    "\tprint(f\"Python version: {platform.python_version()}\")\n",
    "\tprint(f\"PyTorch version: {torch.__version__}\")\n",
    "\tprint(f\"Platform: {platform.platform()}\")\n",
    "\tprint(f\"Machine: {platform.machine()}\")\n",
    "\n",
    "\t# Check device availability\n",
    "\tprint(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\tprint(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "\tif torch.backends.mps.is_available():\n",
    "\t\tprint(\"Running on Apple Silicon with MPS acceleration\")\n",
    "\telif torch.cuda.is_available():\n",
    "\t\tprint(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "\telse:\n",
    "\t\tprint(\"Running on CPU\")\n",
    "\n",
    "\tfine_tune_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fine-tune the Qwen Model\n",
    "\n",
    "Now we'll run the fine-tuning process using our script. This will train the model to understand demographic targeting requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning process...\n",
      "This may take 10-30 minutes depending on your hardware.\n",
      "Using device: mps\n",
      "FP16 enabled: False\n",
      "Device map enabled: False\n",
      "Platform: macOS-15.5-arm64-arm-64bit-Mach-O\n",
      "Trying to load model: Qwen/Qwen3-0.6B\n",
      "Successfully loaded model: Qwen/Qwen3-0.6B\n",
      "Loaded 228 training examples\n",
      "Split data: 182 train, 46 validation\n",
      "Prepared training dataset with 182 examples\n",
      "Prepared validation dataset with 46 examples\n",
      "Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='92' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/92 01:37 < 01:21, 0.50 it/s, Epoch 1.09/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.746200</td>\n",
       "      <td>0.570691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>0.354272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.300700</td>\n",
       "      <td>0.293315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>0.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.163200</td>\n",
       "      <td>0.249995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.196800</td>\n",
       "      <td>0.231329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.252500</td>\n",
       "      <td>0.234547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.224603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>0.216989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.089400</td>\n",
       "      <td>0.206914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during training: [enforce fail at inline_container.cc:664] . unexpected pos 3782341248 vs 3782341136\n",
      "Trying with even smaller batch size...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='182' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 51/182 01:20 < 03:35, 0.61 it/s, Epoch 0.55/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.236518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.227800</td>\n",
       "      <td>0.250816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.196900</td>\n",
       "      <td>0.254704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.272046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>0.284367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.236400</td>\n",
       "      <td>0.249262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.121200</td>\n",
       "      <td>0.241262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>0.240035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.238182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.078400</td>\n",
       "      <td>0.238882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 17:10:41.789 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_40-3517201249‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:41.839 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_41-3546179958‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:41.971 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_41-878773179‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:42.038 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_42-704088224‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:42.120 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_42-3961880923‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:42.205 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_42-2088307286‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:42.287 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_42-2589285672‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:42.369 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_42-269146559‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:42.453 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_42-8668207‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:42.535 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_42-3870661471‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:42.617 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_42-2165561072‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:42.702 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_42-2973439872‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:42.787 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_42-3339780435‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:42.873 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_42-3920137594‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:42.958 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_42-218643995‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:43.041 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_43-3813217407‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:43.123 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_43-3334773557‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:43.210 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_43-2386281520‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:43.300 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_43-317467504‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:43.385 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_43-2307775309‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:43.468 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_43-2281304637‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:43.551 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_43-3778066856‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:43.638 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_43-2807466761‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:43.725 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_43-2057363655‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:43.808 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_43-4232963009‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:43.894 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_43-3219735146‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:43.975 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_43-1138461078‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:44.059 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_44-544586115‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:44.141 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_44-3107015398‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:44.229 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_44-3902739051‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:44.312 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_44-3180013064‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:44.393 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_44-1089269024‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:44.481 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_44-1377069914‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:44.565 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_44-625441695‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:59.229 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_59-2620185400‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:59.325 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_59-595041484‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:59.408 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_59-1260232514‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:59.493 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_59-1469400492‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:59.578 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_59-4026105866‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:59.660 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_59-1432846564‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:59.746 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_59-3302220912‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:59.833 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_59-2828630724‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:10:59.916 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_10_59-882179525‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:44.789 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_44-831836511‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:44.883 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_44-3469355141‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:44.978 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_44-190938973‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:45.073 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_45-3678644893‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:45.169 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_45-2531657000‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:45.263 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_45-294139894‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:45.358 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_45-1128827585‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:49.366 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_49-1849788763‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:49.461 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_49-1476376941‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:49.559 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_49-3121634954‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:49.658 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_49-957837344‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:49.755 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_49-3043059616‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:49.855 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_49-1844641061‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:49.955 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_49-3332423589‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:50.054 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_50-852237213‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:50.157 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_50-498024734‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:50.260 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_50-1233748822‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:50.363 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_50-71551675‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:50.502 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_50-2797368572‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:50.594 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_50-2851197468‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:50.691 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_50-2452074116‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:50.793 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_50-2362557843‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:50.893 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_50-1365090986‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:50.991 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_50-1681020939‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:51.090 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_51-3274091804‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:51.189 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_51-3991579192‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:51.286 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_51-491958413‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:51.387 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_51-2213192848‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:51.486 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_51-887121067‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:51.582 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_51-1999857924‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:51.681 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_51-1818994363‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:51.779 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_51-3484798380‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:51.876 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_51-4153618230‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:51.973 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_51-2009529103‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:52.070 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_52-228717777‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:52.165 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_52-399715368‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:52.261 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_52-3218223335‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n",
      "2025-10-15 17:11:52.363 Python[12073:175050843] Error creating directory \n",
      " The volume ‚ÄúMacintosh HD‚Äù is out of space. You can‚Äôt save the file ‚Äúmpsgraph-12073-2025-10-15_17_11_52-3370226626‚Äù because the volume ‚ÄúMacintosh HD‚Äù is out of space.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second attempt failed: [enforce fail at inline_container.cc:664] . unexpected pos 1504785408 vs 1504785296\n",
      "Error during fine-tuning: [enforce fail at inline_container.cc:664] . unexpected pos 1504785408 vs 1504785296\n",
      "\n",
      "Troubleshooting tips:\n",
      "1. Make sure you have enough GPU memory or use a smaller batch size\n",
      "2. Check that the training data is formatted correctly\n",
      "3. Ensure you have internet access to download the base model\n"
     ]
    }
   ],
   "source": [
    "# Import the fine-tuning function\n",
    "from qwenfinetune import fine_tune_model\n",
    "\n",
    "# Start fine-tuning\n",
    "print(\"Starting fine-tuning process...\")\n",
    "print(\"This may take 10-30 minutes depending on your hardware.\")\n",
    "\n",
    "try:\n",
    "    output_dir = fine_tune_model()\n",
    "    print(f\"Fine-tuning completed successfully. Model saved to: {output_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during fine-tuning: {str(e)}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Make sure you have enough GPU memory or use a smaller batch size\")\n",
    "    print(\"2. Check that the training data is formatted correctly\")\n",
    "    print(\"3. Ensure you have internet access to download the base model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test the Fine-tuned Model\n",
    "\n",
    "Let's verify that our fine-tuned model works correctly before converting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model from ./qwen-demographics-finetuned\n",
      "Model loaded successfully\n",
      "\n",
      "Testing prompt: Target young people in the Rockies making more than 50K\n",
      "Input prompt: Parse this demographic targeting request: Target young people in the Rockies making more than 50K\n",
      "\n",
      "Response:\n",
      "Model output:\n",
      "```json\n",
      "{\n",
      "  \"chain_of_thought\": \"Young people typically means 18-24. Rockies is specified. More than 50K means income brackets 55_70, 70_85, 85_100, 100_125, 125_150, 150_200, 200_250, 250_500, 500_plus.\",\n",
      "  \"ages\": [\n",
      "    \"pop_18_24\"\n",
      "  ],\n",
      "  \"gender\": [\n",
      "    \"All\"\n",
      "  ],\n",
      "  \"income\": [\n",
      "    \"55_70\",\n",
      "    \"70_85\",\n",
      "    \"85_100\",\n",
      "    \"100_125\",\n",
      "    \"125_150\",\n",
      "    \"150_200\",\n",
      "    \"\n",
      "\n",
      "Testing prompt: Target women over 30 in urban areas interested in education\n",
      "Input prompt: Parse this demographic targeting request: Target women over 30 in urban areas interested in education\n",
      "\n",
      "Response:\n",
      "Model output:\n",
      "```json\n",
      "{\n",
      "  \"chain_of_thought\": \"Women over 30 means 30 and above, so ALL age brackets from 30+: pop_35_44, pop_45_54, pop_55_64, pop_65_74, pop_75_plus. Urban areas is explicitly mentioned as an interest. Education is explicitly mentioned as an interest.\",\n",
      "  \"ages\": [\n",
      "    \"pop_35_44\",\n",
      "    \"pop_45_54\",\n",
      "    \"pop_55_64\",\n",
      "    \"pop_65_74\",\n",
      "    \"pop_75_plus\"\n",
      "  ],\n",
      "  \"gender\": [\n",
      "    \"Female\"\n",
      "  ],\n",
      "  \"income\": [],\n",
      "  \"cities\": [],\n",
      "  \"states\": [],\n",
      "  \"zip_codes\": [],\n",
      "  \"dmas\": [],\n",
      "  \"interests\": [\n",
      "    \"education\"\n",
      "  ]\n",
      "}\n",
      "```json\n",
      "```json\n",
      "\n",
      "Testing prompt: Target people who like sports and live in Texas.\n",
      "Input prompt: Parse this demographic targeting request: Target people who like sports and live in Texas.\n",
      "\n",
      "Response:\n",
      "Model output:\n",
      "```json\n",
      "{\n",
      "  \"chain_of_thought\": \"Texas is explicitly mentioned. Sports relates to sports interest category.\",\n",
      "  \"ages\": [],\n",
      "  \"gender\": [\n",
      "    \"All\"\n",
      "  ],\n",
      "  \"income\": [],\n",
      "  \"cities\": [],\n",
      "  \"states\": [\n",
      "    \"Texas\"\n",
      "  ],\n",
      "  \"zip_codes\": [],\n",
      "  \"dmas\": [],\n",
      "  \"interests\": [\n",
      "    \"sports\"\n",
      "  ]\n",
      "}\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n",
      "```json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_path = './qwen-demographics-finetuned'\n",
    "\n",
    "try:\n",
    "    print(f\"Loading fine-tuned model from {model_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "    def test_model(prompt):\n",
    "        \"\"\"Test the fine-tuned model with a prompt\"\"\"\n",
    "        formatted_prompt = f\"Parse this demographic targeting request: {prompt}\\n\\nResponse:\"\n",
    "        print(f\"Input prompt: {formatted_prompt}\")\n",
    "        \n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.6,\n",
    "                top_p =0.95,\n",
    "                top_k=20,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response[len(formatted_prompt):].strip()\n",
    "    \n",
    "    #Test with sample prompts\n",
    "    test_prompts = [\n",
    "        \"Target young people in the Rockies making more than 50K\",\n",
    "        \"Target women over 30 in urban areas interested in education\",\n",
    "        \"Target people who like sports and live in Texas.\"\n",
    "    ]\n",
    "\n",
    "    for prompt in test_prompts:\n",
    "        print(f\"\\nTesting prompt: {prompt}\")\n",
    "        result = test_model(prompt)\n",
    "        print(f\"Model output:\\n{result}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model files not found in {model_path}\")\n",
    "    print(\"Make sure fine-tuning completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Setup llama.cpp\n",
    "\n",
    "To convert our model to GGUF format, we need llama.cpp. Let's download and set it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading llama.cpp repository...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Cloning into 'llama.cpp'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully cloned llama.cpp repository\n",
      "Found convert_hf_to_gguf.py script\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Check if llama.cpp directory exists\n",
    "if not os.path.exists('llama.cpp'):\n",
    "    print(\"Downloading llama.cpp repository...\")\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp\"], check=True)\n",
    "        print(\"Successfully cloned llama.cpp repository\")\n",
    "    except subprocess.SubprocessError as e:\n",
    "        print(f\"Error downloading llama.cpp: {e}\")\n",
    "        print(\"Please install git and try again, or download manually from https://github.com/ggerganov/llama.cpp\")\n",
    "else:\n",
    "    print(\"llama.cpp directory already exists\")\n",
    "\n",
    "# Check if the conversion script exists\n",
    "if os.path.exists('llama.cpp/convert_hf_to_gguf.py'):\n",
    "    print(\"Found convert_hf_to_gguf.py script\")\n",
    "else:\n",
    "    print(\"Warning: Could not find convert_hf_to_gguf.py script\")\n",
    "    print(\"Please ensure you have the latest version of llama.cpp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to GGUF Format\n",
    "\n",
    "Now we'll convert our fine-tuned model to GGUF format for use with Ollama. We'll use the llama.cpp conversion script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to GGUF format...\n",
      "Command: /Users/ericlivesay/.cache/uv/builds-v0/.tmp1Yeg2Y/bin/python llama.cpp/convert_hf_to_gguf.py /Users/ericlivesay/PycharmProjects/finetuneqwen06b/qwen-demographics-finetuned --outfile /Users/ericlivesay/PycharmProjects/finetuneqwen06b/gguf_output/qwen-demographics-finetuned.gguf --outtype f16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful!\n",
      "\n",
      "Generated GGUF files: ['qwen-demographics-finetuned-q8_0.gguf', 'qwen-demographics-finetuned.gguf', 'qwen-demographics-finetuned-f16.gguf', 'qwen-demographics-finetuned-q8_0v2.gguf']\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('./gguf_output', exist_ok=True)\n",
    "\n",
    "# Get absolute paths\n",
    "model_path = os.path.abspath('./qwen-demographics-finetuned')\n",
    "output_path = os.path.abspath('./gguf_output/qwen-demographics-finetuned.gguf')\n",
    "\n",
    "# Define the conversion command with proper paths\n",
    "conversion_script = os.path.join('llama.cpp', 'convert_hf_to_gguf.py')\n",
    "conversion_command = [\n",
    "    sys.executable,  # Use the current Python interpreter\n",
    "    conversion_script,\n",
    "    model_path,\n",
    "    \"--outfile\", output_path,\n",
    "    \"--outtype\", \"f16\"  # Use f16 for better balance of size and quality\n",
    "]\n",
    "\n",
    "print(\"Converting to GGUF format...\")\n",
    "print(\"Command:\", \" \".join(conversion_command))\n",
    "\n",
    "try:\n",
    "    # Check if the conversion script exists\n",
    "    if not os.path.exists(conversion_script):\n",
    "        raise FileNotFoundError(f\"Conversion script not found: {conversion_script}\")\n",
    "        \n",
    "    # Check if the model directory exists\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model directory not found: {model_path}\")\n",
    "    \n",
    "    # Run the conversion\n",
    "    result = subprocess.run(conversion_command, capture_output=True, text=True, check=True)\n",
    "    print(\"Conversion successful!\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "    # List generated GGUF files\n",
    "    gguf_files = [f for f in os.listdir('./gguf_output') if f.endswith('.gguf')]\n",
    "    if gguf_files:\n",
    "        print(f\"Generated GGUF files: {gguf_files}\")\n",
    "    else:\n",
    "        print(\"No GGUF files were created. Please check the conversion output for errors.\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Conversion failed: {e}\")\n",
    "    print(f\"Error output: {e.stderr}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Make sure you have the latest version of llama.cpp\")\n",
    "    print(\"2. Check that all required Python packages are installed\")\n",
    "    print(\"3. Make sure your model was fine-tuned successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Ollama Installation\n",
    "\n",
    "Before proceeding, let's make sure Ollama is installed and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is installed: ollama version is 0.11.11\n",
      "Ollama service is running ✓\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import requests\n",
    "\n",
    "def check_ollama():\n",
    "    \"\"\"Check if Ollama is installed and running\"\"\"\n",
    "    try:\n",
    "        # Check if Ollama is installed\n",
    "        result = subprocess.run([\"ollama\", \"--version\"], capture_output=True, text=True)\n",
    "        print(f\"Ollama is installed: {result.stdout.strip()}\")\n",
    "        \n",
    "        # Check if Ollama service is running by making an API request\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"Ollama service is running ✓\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Ollama service returned status code: {response.status_code}\")\n",
    "            return False\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"Ollama is not installed. Please install it from https://ollama.ai\")\n",
    "        return False\n",
    "    except requests.RequestException:\n",
    "        print(\"Ollama service is not running. Please start it by running 'ollama serve'\")\n",
    "        return False\n",
    "\n",
    "# Check Ollama status\n",
    "ollama_ready = check_ollama()\n",
    "if not ollama_ready:\n",
    "    print(\"\\nPlease install and start Ollama before continuing.\")\n",
    "    print(\"Installation instructions: https://ollama.ai/download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Ollama Model File\n",
    "\n",
    "Create a model file that tells Ollama how to use our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing Modelfile for 'hf.co/Qwen/Qwen3-0.6B-GGUF:Q8_0'...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def show_modelfile(model_name):\n",
    "    \"\"\"\n",
    "    Run the 'ollama show --modelfile' command for a specified model.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the Ollama model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success boolean, output or error message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(\"qwen_demographics_modelfile\", \"w\") as f:\n",
    "            subprocess.call( ['ollama', 'show', '--modelfile', model_name], stdout=f)\n",
    "        return True, result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return False, f\"Command failed with exit code {e.returncode}:\\n{e.stderr}\"\n",
    "    except FileNotFoundError:\n",
    "        return False, \"Error: 'ollama' command not found. Is Ollama installed and in your PATH?\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Unexpected error: {str(e)}\"\n",
    "\n",
    "# Set the model name - change this to view a different model\n",
    "model = \"hf.co/Qwen/Qwen3-0.6B-GGUF:Q8_0\"\n",
    "\n",
    "print(f\"Showing Modelfile for '{model}'...\\n\")\n",
    "success, output = show_modelfile(model)\n",
    "\n",
    "if success:\n",
    "    print(output)\n",
    "else:\n",
    "    print(f\"Error: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the model file to use our fine tuned model:\n",
    "Open the qwen-demographics-finetuned file and editn the FROM line to point to our new model file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated qwen_demographics_modelfile\n"
     ]
    }
   ],
   "source": [
    "# Set the filename of your modelfile\n",
    "filename = \"qwen_demographics_modelfile\"\n",
    "\n",
    "try:\n",
    "    # Read the file content\n",
    "    with open(filename, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Find and replace the FROM line\n",
    "    updated = False\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().startswith(\"FROM\"):\n",
    "            lines[i] = \"FROM ./gguf_output/qwen-demographics-finetuned.gguf\\n\"\n",
    "            updated = True\n",
    "            break\n",
    "    \n",
    "    if not updated:\n",
    "        print(f\"No 'FROM' line found in {filename}\")\n",
    "    else:\n",
    "        # Write the updated content back to the file\n",
    "        with open(filename, \"w\") as file:\n",
    "            file.writelines(lines)\n",
    "        \n",
    "        print(f\"Successfully updated {filename}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "    print(\"Please provide the correct path to the modelfile.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Ollama Model\n",
    "\n",
    "Create a new Ollama model using our fine-tuned GGUF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Ollama model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama model created successfully!\n",
      "\n",
      "Available Ollama models:\n",
      "NAME                                                     ID              SIZE      MODIFIED               \n",
      "qwen-demographics-finetuned:latest                       fbdc9a3039d6    1.2 GB    Less than a second ago    \n",
      "ericqwencpuonly:latest                                   07f472a739df    639 MB    7 days ago                \n",
      "hf.co/ggml-org/gemma-3-270m-GGUF:Q8_0                    4f341f194799    291 MB    2 weeks ago               \n",
      "qwenfinetunedv2:latest                                   b115aa207881    1.2 GB    5 weeks ago               \n",
      "qwenfinetuned_demo_q80:latest                            b653cb22551f    639 MB    5 weeks ago               \n",
      "qwenfinetuned:latest                                     173438e33ad2    1.2 GB    6 weeks ago               \n",
      "qwen-demographics-finetuned-f16.gguf:latest              b5e0b3bc6ec1    1.2 GB    6 weeks ago               \n",
      "hf.co/Qwen/Qwen3-0.6B-GGUF:Q8_0                          3e52e1a23a40    639 MB    6 weeks ago               \n",
      "qwen317bcpu:latest                                       114d949a246b    1.1 GB    6 weeks ago               \n",
      "hf.co/unsloth/Qwen3-1.7B-GGUF:latest                     eae9227d8f88    1.1 GB    6 weeks ago               \n",
      "gemma31bcpu:latest                                       37abc67ed6f9    815 MB    6 weeks ago               \n",
      "gemma3:1b                                                8648f39daa8f    815 MB    6 weeks ago               \n",
      "gemma3cpu:latest                                         31f12befbf55    3.3 GB    7 weeks ago               \n",
      "gemma3:4b                                                a2af6cc3eb7f    3.3 GB    7 weeks ago               \n",
      "hf.co/lm-kit/qwen-3-8b-instruct-gguf:Q4_K_M              6ccf44b1dc89    5.0 GB    7 weeks ago               \n",
      "qwen3cpu:latest                                          8716a54933ab    1.8 GB    7 weeks ago               \n",
      "hf.co/Qwen/Qwen3-1.7B-GGUF:Q8_0                          912a14ba79d9    1.8 GB    7 weeks ago               \n",
      "gpt-oss:20b                                              f2b8351c629c    13 GB     2 months ago              \n",
      "qwen3longcontedt:latest                                  03858ab1a361    17 GB     2 months ago              \n",
      "hf.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF:Q4_K_S    9a56aa9ab42e    17 GB     2 months ago              \n",
      "hf.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF:Q4_K_M    ffa066d9b7e3    18 GB     2 months ago              \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Check if Ollama is running first\n",
    "if not os.path.exists('qwen_demographics_modelfile'):\n",
    "    print(\"Error: Model file not found. Please complete the previous step.\")\n",
    "else:\n",
    "    # Create the Ollama model\n",
    "    print(\"Creating Ollama model...\")\n",
    "    \n",
    "    try:\n",
    "        # Try to delete existing model if it exists\n",
    "        subprocess.run(\n",
    "            [\"ollama\", \"rm\", \"qwen-demographics-finetuned\"],\n",
    "            capture_output=True,\n",
    "            check=False  # Don't fail if model doesn't exist\n",
    "        )\n",
    "        \n",
    "        # Create the new model\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"create\", \"qwen-demographics-finetuned\", \"-f\", \"qwen_demographics_modelfile\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        \n",
    "        print(\"Ollama model created successfully!\")\n",
    "        if result.stdout:\n",
    "            print(result.stdout)\n",
    "        \n",
    "        # List available models to confirm\n",
    "        list_result = subprocess.run(\n",
    "            [\"ollama\", \"list\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\nAvailable Ollama models:\")\n",
    "        print(list_result.stdout)\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to create Ollama model: {e}\")\n",
    "        print(f\"Error output: {e.stderr}\")\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        print(\"1. Ensure Ollama is installed and running\")\n",
    "        print(\"2. Check that the GGUF file exists and is valid\")\n",
    "        print(\"3. Verify the model file is correctly formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Ollama Model\n",
    "\n",
    "Test our deployed model with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the fine-tuned model with Ollama...\n",
      "\n",
      "============================================================\n",
      "Test 1: Target young professionals in California making over 100K\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "<think>\n",
      "Okay, let's see. Young professionals typically means 25-44 age range. California is specified. Over 100K means income brackets 100_125, 125_150, 150_200, 200_250, 250_500, 500_plus.\n",
      "\n",
      "So the final answer should include California and all specified income brackets. No specific interests mentioned, so interests array should be empty.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"chain_of_thought\": \"California is explicitly mentioned. Young professionals typically means 25-44 age range. Over 100K means income brackets 100_125, 125_150, 150_200, 200_250, 250_500, 500_plus.\",\n",
      "  \"ages\": [\n",
      "    \"pop_25_34\",\n",
      "    \"pop_35_44\"\n",
      "  ],\n",
      "  \"gender\": [\n",
      "    \"All\"\n",
      "  ],\n",
      "  \"income\": [\n",
      "    \"100_125\",\n",
      "    \"125_150\",\n",
      "    \"150_200\",\n",
      "    \"200_250\",\n",
      "    \"250_500\",\n",
      "    \"500_plus\"\n",
      "  ],\n",
      "  \"cities\": [],\n",
      "  \"states\": [\n",
      "    \"California\"\n",
      "  ],\n",
      "  \"zip_codes\": [],\n",
      "  \"dmas\": [],\n",
      "  \"interests\": []\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "✓ Response contains valid JSON\n",
      "\n",
      "============================================================\n",
      "Test 2: Target wealthy seniors in Florida who like golf\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "<think>\n",
      "Okay, let's tackle this query: Target wealthy seniors in Florida who like golf.\n",
      "\n",
      "First, I need to parse the age range. Since it's mentioned as \"seniors,\" typically means 65+ (pop_65_74, pop_75_plus). \n",
      "\n",
      "Florida is explicitly mentioned. Golf relates to sports interest category.\n",
      "\n",
      "So putting it all together: Age brackets pop_65_74 and pop_75_plus in Florida. Sports interest category.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"chain_of_thought\": \"Florida is specified. Golf relates to sports interest category.\",\n",
      "  \"ages\": [\n",
      "    \"pop_65_74\",\n",
      "    \"pop_75_plus\"\n",
      "  ],\n",
      "  \"gender\": [\n",
      "    \"All\"\n",
      "  ],\n",
      "  \"income\": [],\n",
      "  \"cities\": [],\n",
      "  \"states\": [\n",
      "    \"Florida\"\n",
      "  ],\n",
      "  \"zip_codes\": [],\n",
      "  \"dmas\": [],\n",
      "  \"interests\": [\n",
      "    \"sports\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "✓ Response contains valid JSON\n",
      "\n",
      "============================================================\n",
      "Test 3: Target middle-aged families in the Midwest with kids\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "<think>\n",
      "Okay, let's see. Middle-aged typically means 25-64 age range. Families is explicitly mentioned. No specific interests mentioned, so I should leave interests empty.\n",
      "\n",
      "Response:\n",
      "```json\n",
      "{\n",
      "  \"chain_of_thought\": \"Middle-aged is 25-64. Families relates to family_and_relationships interest category.\",\n",
      "  \"ages\": [\n",
      "    \"pop_25_34\",\n",
      "    \"pop_35_44\",\n",
      "    \"pop_45_54\",\n",
      "    \"pop_55_64\"\n",
      "  ],\n",
      "  \"gender\": [\n",
      "    \"All\"\n",
      "  ],\n",
      "  \"income\": [],\n",
      "  \"cities\": [],\n",
      "  \"states\": [],\n",
      "  \"zip_codes\": [],\n",
      "  \"dmas\": [],\n",
      "  \"interests\": [\n",
      "    \"family_and_relationships\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "✓ Response contains valid JSON\n",
      "\n",
      "============================================================\n",
      "You can now interact with your model using:\n",
      "ollama run qwen-demographics\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test the model with demographic targeting prompts\n",
    "test_prompts = [\n",
    "    \"Target young professionals in California making over 100K\",\n",
    "    \"Target wealthy seniors in Florida who like golf\", \n",
    "    \"Target middle-aged families in the Midwest with kids\"\n",
    "]\n",
    "\n",
    "print(\"Testing the fine-tuned model with Ollama...\")\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test {i+1}: {prompt}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"qwen-demographics-finetuned\", f\"Parse this demographic targeting request: {prompt}\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True,\n",
    "            timeout=60\n",
    "        )\n",
    "        \n",
    "        print(\"Response:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Try to parse the response as JSON\n",
    "        try:\n",
    "            # Extract JSON part from the response\n",
    "            response_text = result.stdout.strip()\n",
    "            # Try to find JSON in the response\n",
    "            import re\n",
    "            json_match = re.search(r'\\{[^}]*\\}', response_text)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(0)\n",
    "                json_obj = json.loads(json_str)\n",
    "                print(\"\\n✓ Response contains valid JSON\")\n",
    "            else:\n",
    "                print(\"\\n⚠️ Could not find JSON in response\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"\\n⚠️ Response is not valid JSON\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"Error output: {e.stderr}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"Request timed out (>60s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"You can now interact with your model using:\")\n",
    "print(\"ollama run qwen-demographics\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "🎉 **Congratulations!** You've successfully completed the entire workflow:\n",
    "\n",
    "### ✅ What We Accomplished:\n",
    "1. **Fine-tuned** a Qwen 0.6B model for demographic targeting\n",
    "2. **Converted** the model to GGUF format for efficient inference\n",
    "3. **Created** an Ollama model file pointing to your fine-tuned model\n",
    "4. **Deployed** the model with Ollama for local inference\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "- **Improve Training Data**: Add more diverse examples to improve model performance\n",
    "- **Experiment with Quantization**: Try different levels (Q4_0, Q8_0) for size/performance trade-offs\n",
    "- **Production Integration**: Use the model in your application\n",
    "- **Performance Monitoring**: Track model performance and retrain as needed\n",
    "- **Scale Up**: Consider larger models if you need better performance\n",
    "\n",
    "### 📋 Command Reference:\n",
    "\n",
    "```bash\n",
    "# Fine-tune the model\n",
    "python qwenfinetune.py\n",
    "\n",
    "# Convert to GGUF format\n",
    "python llama.cpp/convert_hf_to_gguf.py ./qwen-demographics-finetuned --outdir ./gguf_output --outtype f16\n",
    "\n",
    "# Update the modelfile to point the FROM line to FROM ./gguf_output/qwen-demographics-finetuned.gguf\n",
    "\n",
    "# Create new Ollama model\n",
    "ollama create qwen-demographics -f qwen_demographics_modelfile\n",
    "\n",
    "# Run the model\n",
    "ollama run qwen-demographics\n",
    "```\n",
    "\n",
    "### 💡 Key Benefits of This Approach:\n",
    "- **Local Control**: You own the model weights and can run inference locally\n",
    "- **Cost Effective**: No API costs after initial training\n",
    "- **Privacy**: Data stays on your local machine\n",
    "- **Customization**: Model is specifically trained for your demographic targeting use case\n",
    "- **Small Footprint**: 0.6B model runs efficiently on consumer hardware\n",
    "\n",
    "The model is now ready to be integrated into your application for demographic targeting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
